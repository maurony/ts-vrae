{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c62bc958ea6e0d2dc0790a4f67a189582a93979c94e888c8679a6e8ac8502601"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Explain SMAVRA Predictions\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"4d8ddb41e7f340c182a6a62699502d9f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "latent_dir = os.path.join(\"data/output/explain/latent\", RUN_ID)\n",
    "latents = []\n",
    "relevant_files = [p for p in Path(latent_dir).iterdir() if int(os.path.basename(p)[:8]) > 20201000]\n",
    "for p in relevant_files:\n",
    "    table = pq.read_table(p)\n",
    "    latents.append(table.to_pandas())\n",
    "df = pd.concat(latents, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"epoch_loss\"] = df.epoch_loss.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cols = [f\"latent_{i}\" for i in range(16)] + [\"epoch_loss\"]\n",
    "train_df = df\n",
    "\n",
    "scaled = RobustScaler().fit_transform(train_df.loc[:,cols].values)\n",
    "\n",
    "train_df.loc[:,cols] = scaled\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=100, gen_min_span_tree=True)\n",
    "clusterer.fit(train_df.loc[:,cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "classification = pd.DataFrame({\"labels\":clusterer.labels_, \"probs\": clusterer.probabilities_})\n",
    "\n",
    "train_df[\"labels\"] = clusterer.labels_\n",
    "train_df[\"probs\"] = clusterer.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(train_df)\n",
    "# create df for visualization\n",
    "pca_columns = [f\"PC{i+1}\" for i in range(3)]\n",
    "components = pd.DataFrame(\n",
    "    components, columns=pca_columns\n",
    ").reset_index()\n",
    "components = pd.concat(\n",
    "    [train_df.reset_index(), components], axis=1)\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "labels = {str(i): f\"PC {i+1}\" for i in range(3)}\n",
    "labels['color'] = 'log(epoch_loss)'\n",
    "# fit latent\n",
    "pca_fig = px.scatter_matrix(\n",
    "    components,\n",
    "    color=train_df.labels.astype(\"str\"),\n",
    "    dimensions=pca_columns,\n",
    "    labels=labels,\n",
    "    title=f'Run: {RUN_ID}; Total Explained Variance: {total_var:.2f}%',\n",
    "    hover_name=\"file_name\",\n",
    "    hover_data=[\"epoch_loss\", \"epoch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "classification = pd.DataFrame({\"labels\":clusterer.labels_, \"props\": clusterer.probabilities_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.groupby(\"labels\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dir = os.path.join(\"data/output/score/\", RUN_ID)\n",
    "score = []\n",
    "for p in Path(score_dir).iterdir():\n",
    "    df = pq.read_table(\n",
    "        p,\n",
    "        columns=[\n",
    "            \"epoch_id\",\n",
    "            \"epoch_mse\",\n",
    "            \"mask_press_se\",\n",
    "            \"resp_flow_se\",\n",
    "            \"delivered_volum_se\"\n",
    "        ]).to_pandas()\n",
    "    \n",
    "    df = df \\\n",
    "        .groupby(\"epoch_id\") \n",
    "\n",
    "    \n",
    "    means = df \\\n",
    "        .rolling(150) \\\n",
    "        [\"mask_press_se\",\"resp_flow_se\",\"delivered_volum_se\"] \\\n",
    "        .mean()\n",
    "    \n",
    "    stds = df \\\n",
    "        .rolling(150) \\\n",
    "        [\"mask_press_se\",\"resp_flow_se\",\"delivered_volum_se\"] \\\n",
    "        .std()\n",
    "    \n",
    "    mins = df \\\n",
    "        .rolling(150) \\\n",
    "        [\"mask_press_se\",\"resp_flow_se\",\"delivered_volum_se\"] \\\n",
    "        .min()\n",
    "    \n",
    "    maxs = df \\\n",
    "        .rolling(150) \\\n",
    "        [\"mask_press_se\",\"resp_flow_se\",\"delivered_volum_se\"] \\\n",
    "        .max()\n",
    "    \n",
    "    mean_cols = [f\"rmean_{c}\" for c in [\"mask_press_se\",\"resp_flow_se\",      \"delivered_volum_se\"]]\n",
    "\n",
    "    std_cols = [f\"rstd_{c}\" for c in [\"mask_press_se\",\"resp_flow_se\",      \"delivered_volum_se\"]]\n",
    "\n",
    "    min_cols = [f\"rmin_{c}\" for c in [\"mask_press_se\",\"resp_flow_se\",      \"delivered_volum_se\"]]\n",
    "\n",
    "    max_cols = [f\"rmax_{c}\" for c in [\"mask_press_se\",\"resp_flow_se\",      \"delivered_volum_se\"]]\n",
    "\n",
    "\n",
    "    df = pd.concat([means, stds, mins, maxs], axis = 1).dropna()\n",
    "\n",
    "    df.columns = mean_cols + std_cols + min_cols + max_cols\n",
    "\n",
    "\n",
    "    means = df \\\n",
    "        .groupby(\"epoch_id\") \\\n",
    "        .mean()\n",
    "\n",
    "    stds = df \\\n",
    "        .groupby(\"epoch_id\") \\\n",
    "        .std()\n",
    "\n",
    "    maxs = df \\\n",
    "        .groupby(\"epoch_id\") \\\n",
    "        .max()\n",
    "    \n",
    "    mins = df \\\n",
    "        .groupby(\"epoch_id\") \\\n",
    "        .max()\n",
    "\n",
    "    df = pd.concat([means, stds, mins, maxs], axis = 1).dropna() \n",
    "  \n",
    "        # .rolling(75) \\\n",
    "        # .agg(\n",
    "        #     #mean_epoch_mse=pd.NamedAgg(\"epoch_mse\", \"mean\"),\n",
    "        #     # mean_mask_press=pd.NamedAgg(\"mask_press_se\", \"mean\"), \n",
    "        #     # mean_resp_flow=pd.NamedAgg(\"resp_flow_se\", \"mean\"), \n",
    "        #     # mean_delivered_volum=pd.NamedAgg(\"delivered_volum_se\", \"mean\"), \n",
    "        #     mean_mask_press_se=pd.NamedAgg(\"mask_press_se\", \"mean\"), \n",
    "        #     mean_resp_flow_se=pd.NamedAgg(\"resp_flow_se\", \"mean\"), \n",
    "        #     mean_delivered_volum_se=pd.NamedAgg(\"delivered_volum_se\", \"mean\"),\n",
    "        #     #min_epoch_mse=pd.NamedAgg(\"epoch_mse\", \"min\"), \n",
    "        #     min_mask_press_se=pd.NamedAgg(\"mask_press_se\", \"min\"), \n",
    "        #     min_resp_flow_se=pd.NamedAgg(\"resp_flow_se\", \"min\"), \n",
    "        #     min_delivered_volum_se=pd.NamedAgg(\"delivered_volum_se\", \"min\"),\n",
    "        #     #max_epoch_mse=pd.NamedAgg(\"epoch_mse\", \"max\"),\n",
    "        #     max_mask_press_se=pd.NamedAgg(\"mask_press_se\", \"max\"), \n",
    "        #     max_resp_flow_se=pd.NamedAgg(\"resp_flow_se\", \"max\"), \n",
    "        #     max_delivered_volum_se=pd.NamedAgg(\"delivered_volum_se\", \"max\"),\n",
    "        #     #std_epoch_mse=pd.NamedAgg(\"epoch_mse\", \"std\"), \n",
    "        #     std_mask_press_se=pd.NamedAgg(\"mask_press_se\", \"std\"), \n",
    "        #     std_resp_flow_se=pd.NamedAgg(\"resp_flow_se\", \"std\"), \n",
    "        #     std_delivered_volum_se=pd.NamedAgg(\"delivered_volum_se\", \"std\")\n",
    "        # ).reset_index()\n",
    "    score.append(df)\n",
    "df = pd.concat(score, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_df = df.reset_index().iloc[:, 1:]\n",
    "\n",
    "scaled = RobustScaler().fit_transform(train_df.values)\n",
    "\n",
    "train_df.iloc[:,:] = scaled\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, gen_min_span_tree=True)\n",
    "clusterer.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "classification = pd.DataFrame({\"labels\":clusterer.labels_, \"probs\": clusterer.probabilities_})\n",
    "\n",
    "train_df[\"labels\"] = clusterer.labels_\n",
    "train_df[\"probs\"] = clusterer.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pca = PCA(n_components=3)\n",
    "train_df = train_df[train_df[\"labels\"].values > -1]\n",
    "components = pca.fit_transform(train_df)\n",
    "# create df for visualization\n",
    "pca_columns = [f\"PC{i+1}\" for i in range(3)]\n",
    "components = pd.DataFrame(\n",
    "    components, columns=pca_columns\n",
    ").reset_index()\n",
    "components = pd.concat(\n",
    "    [train_df.reset_index(), components], axis=1)\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "labels = {str(i): f\"PC {i+1}\" for i in range(3)}\n",
    "labels['color'] = 'log(epoch_loss)'\n",
    "# fit latent\n",
    "px.scatter_matrix(\n",
    "    components,\n",
    "    color=train_df.labels.astype(\"str\"),\n",
    "    dimensions=pca_columns,\n",
    "    labels=labels,\n",
    "    title=f'Run: {RUN_ID}; Total Explained Variance: {total_var:.2f}%'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_val = df[\"resp_flow_se\"].values\n",
    "p = np.percentile(col_val, 95)\n",
    "upper_limit = np.median(col_val) + (4 * p)\n",
    "stats.tmean(col_val, limits=[0,upper_limit])\n",
    "stats.tstd(col_val, limits=[0,upper_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "for c in df.columns:\n",
    "    col_val = df[c].values\n",
    "    p = np.percentile(col_val, 95)\n",
    "    upper_limit = np.median(col_val) + (4 * p)\n",
    "    mu = stats.tmean(col_val, limits=[0,upper_limit])\n",
    "    sd = stats.tstd(col_val, limits=[0,upper_limit])\n",
    "\n",
    "    means.append(mu)\n",
    "    stds.append(sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dir = os.path.join(\"data/output/score/\", RUN_ID)\n",
    "score = []\n",
    "for p in Path(score_dir).iterdir():\n",
    "    df = pq.read_table(\n",
    "        p,\n",
    "        columns=[\n",
    "            \"delivered_volum\",\n",
    "            \"epoch_mse\"\n",
    "        ]).to_pandas()\n",
    "    df = df.loc[df[\"delivered_volum\"] >-32760, :]\n",
    "    score.append(df)\n",
    "df = pd.concat(score, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.delivered_volum.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.histogram(df.delivered_volum.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.percentile(df.delivered_volum.values, 99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.groupby(\"epoch_id\").agg({\"epoch_mse\":\"mean\", \"mask_press_se\": \"mean\", \"resp_flow_se\": \"mean\", \"delivered_volum_se\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "49318*750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"4377a3ad68e84162827255bc1a0b7e40\"\n",
    "mlflow_client = MlflowClient()\n",
    "# get run to be explained\n",
    "data = mlflow_client.get_run(RUN_ID).data\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client.get_experiment_by_name(\"SMAVRA\").experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization import visualize as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, tsne = viz.plot_latent(run_id=RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.update_layout(\n",
    "    width=1000,\n",
    "    height=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.update_layout(\n",
    "    width=1000,\n",
    "    height=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION = \"20200930_120001\"\n",
    "EPOCH = 361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = viz.epoch_attention(\n",
    "    run_id=RUN_ID,\n",
    "    session=SESSION,\n",
    "    epoch_nr=EPOCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.imshow(attention[1])\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "scored_path = Path(os.path.join(\"data\", \"output\", \"score\", RUN_ID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "df = pq.read_table(\n",
    "    os.path.join(scored_path, f\"{SESSION}_0_HRD.edf.parquet\")\n",
    ").to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "z_scores = (df[\"epoch_mse\"].values - stats.trim_mean(df[\"epoch_mse\"].values, 0.05)) / stats.tstd(df[\"epoch_mse\"].values, limits=[0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(z_scores[np.where(z_scores < 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_plot = viz.plot_signals(\n",
    "    session=SESSION,\n",
    "    df=df\n",
    ")\n",
    "\n",
    "ts_plot.update_layout(\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    title=f\"Session {SESSION}; Epoch {EPOCH}\"\n",
    ")\n",
    "\n",
    "ts_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}