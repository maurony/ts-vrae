runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
Sys.getenv('ENDPOINT')
Sys.getenv('END_POINT')
Sys.getenv(x = c("END_POINT"))
Sys.getenv(x = c("END_POINT"))
Sys.getenv(x = c("END_POINT"))
Sys.getenv(x = c("END_POINT"))
Sys.getenv("END_POINT")
print(Sys.getenv("END_POINT_KEY"))
Sys.getenv("END_POINT_KEY")
Sys.getenv()
Sys.getenv()
Sys.getenv()
Sys.getenv("END_POINT_KEY")
shiny::runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
shiny::runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp()
library(shiny)
library(DT)
library(ggplot2)
library(plotly)
library(shinythemes)
library(shinydashboard)
library(shinymanager)
# load your dependencies
files_sources = list.files('./R', full.names = T)
sapply(files_sources, source)
# inactivity script
inactivity <- "
function idleTimer() {
var t = setTimeout(logout, 120000);
window.onmousemove = resetTimer; // catches mouse movements
window.onmousedown = resetTimer; // catches mouse movements
window.onclick = resetTimer;     // catches mouse clicks
window.onscroll = resetTimer;    // catches scrolling
window.onkeypress = resetTimer;  //catches keyboard actions
function logout() {
window.close();  //close the window
}
function resetTimer() {
clearTimeout(t);
t = setTimeout(logout, 120000);  // time is in milliseconds (1000 is 1 second)
}
}
idleTimer();"
# data.frame with credentials info
credentials <- data.frame(
user = c("yves"),
password = c("yves"),
# comment = c("alsace", "auvergne", "bretagne"), %>%
stringsAsFactors = FALSE
)
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp()
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer/aitrainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/coding/ts_annotation/AI-Trainer')
# load libraries
require(edf)
require(foreach)
require(doSNOW)
require(parallel)
require(progress)
# --------------------------------------------------------------------------
# Set up data directory structure
# --------------------------------------------------------------------------
device <- 'resmed'
time_dir <- format(Sys.Date(), '%Y%m%d')
dir.create(file.path('data', device, 'classification', time_dir), showWarnings = F)
# --------------------------------------------------------------------------
# List stations
# --------------------------------------------------------------------------
stations <- list.dirs(file.path(getwd(), 'data', device, 'raw'), full.names = F, recursive = F)
station <- stations[1]
# --------------------------------------------------------------------------
edf_dir = file.path(getwd(), 'data', device, 'raw', station)
edf_file_paths <- list.files(edf_dir, recursive = T, pattern = '*.edf')
file_name = edf_file_paths[1]
edf_file <- edf::read.edf(filename = file.path(edf_dir, file_path), read.annotations = F)
# get global header information
header_global <- t(unlist(edf_file$header.global))
#record signal source file information
header_global <- cbind(header_global, file_name, file_path)
# get datetime of signal
dt_signal <- strptime(paste(header_global[1,4], ' ', header_global[1,5], sep = ''), '%d.%m.%y %H.%M.%S')
# get signal header information
header_signal <- t(sapply(edf_file$header.signal, function(x) {
unlist(x)
}))
#record signal source file information
header_signal <- cbind(header_signal, file_name, file_path)
# get the time the signales were recoded
signal_time <- sapply(edf_file$signal, function(x) {
x$t
})
# rename columns to distiguish between time and data later
colnames(signal_time) <- paste(colnames(signal_time), 'T', sep = '_')
# get signal data
signal_data <- sapply(edf_file$signal, function(x) {
x$data
})
signal_ts = format(signal_time[,1] + dt_signal, '%Y-%m-%d %H:%M:%OS4')
# rename to signal data
colnames(signal_data) <- paste(colnames(signal_data), 'DATA', sep = '_')
# combine signal time and signal data and record source file information
signal <- cbind(TimeStamp = signal_ts, signal_time, signal_data, file_name, file_path)
# convert from edf format to list
edf_file <- edf::read.edf(filename = file.path(edf_dir, file_path), read.annotations = F)
# convert from edf format to list
edf_file <- edf::read.edf(filename = file.path(edf_dir, file_name), read.annotations = F)
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
# convert from edf format to list
edf_file <- edf::read.edf(filename = file.path(edf_dir, file_name), read.annotations = F)
# load libraries
require(edf)
require(foreach)
require(doSNOW)
require(parallel)
require(progress)
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
# --------------------------------------------------------------------------
# Set up data directory structure
# --------------------------------------------------------------------------
device <- 'resmed'
time_dir <- format(Sys.Date(), '%Y%m%d')
dir.create(file.path('data', device, 'classification', time_dir), showWarnings = F)
# --------------------------------------------------------------------------
# List stations
# --------------------------------------------------------------------------
stations <- list.dirs(file.path(getwd(), 'data', device, 'raw'), full.names = F, recursive = F)
station <- stations[1]
# --------------------------------------------------------------------------
# List the files to extract
# --------------------------------------------------------------------------
edf_dir = file.path(getwd(), 'data', device, 'raw', station)
edf_file_paths <- list.files(edf_dir, recursive = T, pattern = '*.edf')
file_name = edf_file_paths[1]
# convert from edf format to list
edf_file <- edf::read.edf(filename = file.path(edf_dir, file_name), read.annotations = F)
# get global header information
header_global <- t(unlist(edf_file$header.global))
#record signal source file information
header_global <- cbind(header_global, file_name, file_path)
# get datetime of signal
dt_signal <- strptime(paste(header_global[1,4], ' ', header_global[1,5], sep = ''), '%d.%m.%y %H.%M.%S')
#record signal source file information
header_global <- cbind(header_global, file_name, file_name)
# get datetime of signal
dt_signal <- strptime(paste(header_global[1,4], ' ', header_global[1,5], sep = ''), '%d.%m.%y %H.%M.%S')
# get signal header information
header_signal <- t(sapply(edf_file$header.signal, function(x) {
unlist(x)
}))
#record signal source file information
header_signal <- cbind(header_signal, file_name, file_path)
#record signal source file information
header_signal <- cbind(header_signal, file_name, file_name)
# get the time the signales were recoded
signal_time <- sapply(edf_file$signal, function(x) {
x$t
})
# rename columns to distiguish between time and data later
colnames(signal_time) <- paste(colnames(signal_time), 'T', sep = '_')
# get signal data
signal_data <- sapply(edf_file$signal, function(x) {
x$data
})
signal_ts = format(signal_time[,1] + dt_signal, '%Y-%m-%d %H:%M:%OS4')
# rename to signal data
colnames(signal_data) <- paste(colnames(signal_data), 'DATA', sep = '_')
signal_ts
signal_data
# combine signal time and signal data and record source file information
signal <- cbind(TimeStamp = signal_ts, signal_time, signal_data, file_name, file_path)
signal
signal <- cbind(TimeStamp = signal_ts, signal_time, signal_data, file_name)
head(signal)
# combine signal time and signal data and record source file information
signal <- cbind(TimeStamp = signal_ts, signal_time, signal_data, file_name, StationName=station)
head(signal)
dir.create(file.path('data', device, 'staging', time_dir), showWarnings = F)
target_path, paste(file_name,  'csv', sep = '.')
target_path
target_path <- file.path('data', device, 'staging', time_dir)
dir.create(target_path, showWarnings = F)
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
# --------------------------------------------------------------------------
# Set up data directory structure
# --------------------------------------------------------------------------
device <- 'resmed'
time_dir <- format(Sys.Date(), '%Y%m%d')
target_path <- file.path('data', device, 'staging', time_dir)
dir.create(target_path, showWarnings = F)
target_path
target_path <- file.path('./data', device, 'staging', time_dir)
dir.create(target_path, showWarnings = F)
dir.create(target_path, showWarnings = T)
# --------------------------------------------------------------------------
# List stations
# --------------------------------------------------------------------------
stations <- list.dirs(file.path(getwd(), 'data', device, 'raw'), full.names = F, recursive = F)
getwd()
dir.create(target_path, showWarnings = T, recursive = T)
file.path(target_path, paste(file_name,  'csv', sep = '.')
)
# load libraries
require(edf)
require(foreach)
require(doSNOW)
require(parallel)
require(progress)
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
# --------------------------------------------------------------------------
# Set up data directory structure
# --------------------------------------------------------------------------
device <- 'resmed'
time_dir <- format(Sys.Date(), '%Y%m%d')
target_path <- file.path('./data', device, 'staging', time_dir)
dir.create(target_path, showWarnings = F, recursive = T)
# --------------------------------------------------------------------------
# List stations
# --------------------------------------------------------------------------
stations <- list.dirs(file.path(getwd(), 'data', device, 'raw'), full.names = F, recursive = F)
for (station in stations) {
# --------------------------------------------------------------------------
# List the files to extract
# --------------------------------------------------------------------------
edf_dir = file.path(getwd(), 'data', device, 'raw', station)
edf_file_paths <- list.files(edf_dir, recursive = T, pattern = '*.edf')
# --------------------------------------------------------------------------
# Set up cluster for parallel execution (incl. progress bar)
# --------------------------------------------------------------------------
# create progress bar
pb <- progress_bar$new(
format = "  [:bar] :percent eta: :eta",
total = length(edf_file_paths[is_data_log_file]),
clear = F,
force = T
)
prop_progress <- function() { pb$tick() }
# set num_cores if it is not set by the user
num_cores <- detectCores()
num_cores_ignore <- 1
# create local cluster
cl <- makeCluster(num_cores - num_cores_ignore, type = "SOCK")
registerDoSNOW(cl)
clusterEvalQ(cl, {
library(edf)
})
# --------------------------------------------------------------------------
# read and extract information of edf files
# --------------------------------------------------------------------------
result <- foreach (file_name = edf_file_paths, .options.snow = list(progress=prop_progress)) %dopar% {
# convert from edf format to list
edf_file <- edf::read.edf(filename = file.path(edf_dir, file_name), read.annotations = F)
# get global header information
header_global <- t(unlist(edf_file$header.global))
#record signal source file information
header_global <- cbind(header_global, file_name, file_name)
# get datetime of signal
dt_signal <- strptime(paste(header_global[1,4], ' ', header_global[1,5], sep = ''), '%d.%m.%y %H.%M.%S')
# get signal header information
header_signal <- t(sapply(edf_file$header.signal, function(x) {
unlist(x)
}))
#record signal source file information
header_signal <- cbind(header_signal, file_name, file_name)
# get the time the signales were recoded
signal_time <- sapply(edf_file$signal, function(x) {
x$t
})
# rename columns to distiguish between time and data later
colnames(signal_time) <- paste(colnames(signal_time), 'T', sep = '_')
# get signal data
signal_data <- sapply(edf_file$signal, function(x) {
x$data
})
signal_ts = format(signal_time[,1] + dt_signal, '%Y-%m-%d %H:%M:%OS4')
# rename to signal data
colnames(signal_data) <- paste(colnames(signal_data), 'DATA', sep = '_')
# combine signal time and signal data and record source file information
signal <- cbind(TimeStamp = signal_ts, signal_time, signal_data, file_name, StationName=station)
# signal
write.csv(
x = signal,
file = file.path(target_path, paste(file_name,  'csv', sep = '.')),
row.names = F
)
list (
file = file_path,
processing_time = Sys.time()
)
}
# close cluster
stopCluster(cl)
}
# load libraries
require(edf)
require(foreach)
require(doSNOW)
require(parallel)
require(progress)
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
# --------------------------------------------------------------------------
# Set up data directory structure
# --------------------------------------------------------------------------
device <- 'resmed'
time_dir <- format(Sys.Date(), '%Y%m%d')
target_path <- file.path('./data', device, 'staging', time_dir)
dir.create(target_path, showWarnings = F, recursive = T)
# --------------------------------------------------------------------------
# List stations
# --------------------------------------------------------------------------
stations <- list.dirs(file.path(getwd(), 'data', device, 'raw'), full.names = F, recursive = F)
for (station in stations) {
# --------------------------------------------------------------------------
# List the files to extract
# --------------------------------------------------------------------------
edf_dir = file.path(getwd(), 'data', device, 'raw', station)
edf_file_paths <- list.files(edf_dir, recursive = T, pattern = '*.edf')
# --------------------------------------------------------------------------
# Set up cluster for parallel execution (incl. progress bar)
# --------------------------------------------------------------------------
# create progress bar
pb <- progress_bar$new(
format = "  [:bar] :percent eta: :eta",
total = length(edf_file_paths),
clear = F,
force = T
)
prop_progress <- function() { pb$tick() }
# set num_cores if it is not set by the user
num_cores <- detectCores()
num_cores_ignore <- 1
# create local cluster
cl <- makeCluster(num_cores - num_cores_ignore, type = "SOCK")
registerDoSNOW(cl)
clusterEvalQ(cl, {
library(edf)
})
# --------------------------------------------------------------------------
# read and extract information of edf files
# --------------------------------------------------------------------------
result <- foreach (file_name = edf_file_paths, .options.snow = list(progress=prop_progress)) %dopar% {
# convert from edf format to list
edf_file <- edf::read.edf(filename = file.path(edf_dir, file_name), read.annotations = F)
# get global header information
header_global <- t(unlist(edf_file$header.global))
#record signal source file information
header_global <- cbind(header_global, file_name, file_name)
# get datetime of signal
dt_signal <- strptime(paste(header_global[1,4], ' ', header_global[1,5], sep = ''), '%d.%m.%y %H.%M.%S')
# get signal header information
header_signal <- t(sapply(edf_file$header.signal, function(x) {
unlist(x)
}))
#record signal source file information
header_signal <- cbind(header_signal, file_name, file_name)
# get the time the signales were recoded
signal_time <- sapply(edf_file$signal, function(x) {
x$t
})
# rename columns to distiguish between time and data later
colnames(signal_time) <- paste(colnames(signal_time), 'T', sep = '_')
# get signal data
signal_data <- sapply(edf_file$signal, function(x) {
x$data
})
signal_ts = format(signal_time[,1] + dt_signal, '%Y-%m-%d %H:%M:%OS4')
# rename to signal data
colnames(signal_data) <- paste(colnames(signal_data), 'DATA', sep = '_')
# combine signal time and signal data and record source file information
signal <- cbind(TimeStamp = signal_ts, signal_time, signal_data, file_name, StationName=station)
# signal
write.csv(
x = signal,
file = file.path(target_path, paste(file_name,  'csv', sep = '.')),
row.names = F
)
list (
file = file_path,
processing_time = Sys.time()
)
}
# close cluster
stopCluster(cl)
}
header_signal
file_name = edf_file_paths[1]
file_name
unlink(target_path, recursive = T, force = T)
dir.create(target_path, showWarnings = F, recursive = T)
# load libraries
require(edf)
require(foreach)
require(doSNOW)
require(parallel)
require(progress)
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
# --------------------------------------------------------------------------
# Set up data directory structure
# --------------------------------------------------------------------------
device <- 'resmed'
time_dir <- format(Sys.Date(), '%Y%m%d')
target_path <- file.path('./data', device, 'staging', time_dir)
unlink(target_path, recursive = T, force = T)
dir.create(target_path, showWarnings = F, recursive = T)
# --------------------------------------------------------------------------
# List stations
# --------------------------------------------------------------------------
stations <- list.dirs(file.path(getwd(), 'data', device, 'raw'), full.names = F, recursive = F)
for (station in stations) {
print(paste('Processing station:', station))
print(paste('Processing station:', station))
print(paste('Processing station:', station))
print(paste('Processing station:', station))
print(paste('Processing station:', station))
print(paste('Processing station:', station))
}
shiny::runApp('C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemreich/models/rnn_anomaly/ai_trainer')
list.dirs('./
')
list.dirs('./')
list.dirs(data_dir)
date_stamp <- "20200218"
data_dir <- '../data/resmed/classification'
list.dirs(data_dir)
getwd()
wd = 'C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemreich/models/rnn_anomaly/ai_trainer'
data_dir <- '../data/resmed/classification'
list.dirs(data_dir)
list.dirs(file.path(wd, data_dir))
list.dirs(file.path(wd, data_dir), recursive = F, full.names = F)
list.dirs(file.path(wd, data_dir), recursive = F, full.names = F)
file.path(wd, data_dir)
list.dirs(file.path(wd, data_dir))
list.dirs(file.path(wd, data_dir), recursive = F, full.names = F)
wd
data_dir
list.dirs(file.path(wd, data_dir), recursive = F)
list.dirs(file.path(wd, data_dir), recursive = F, full.names = F)
list.files(file.path(wd, data_dir), recursive = F, full.names = F)
list.files(file.path(wd, data_dir), recursive = T, full.names = F)
list.files(file.path(wd, data_dir), recursive = F, full.names = F)
data_dir
file.path(wd, data_dir)
wd = 'C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer/models/rnn_anomaly/ai_trainer'
date_stamp <- "20200218"
data_dir <- '../data/resmed/classification'
list.files(file.path(wd, data_dir), recursive = F, full.names = F)
wd
wd = 'C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer/ai_trainer'
date_stamp <- "20200218"
data_dir <- '../data/resmed/classification'
unclassified_path <- file.path(data_dir, date_stamp, 'unclassified')
wd = 'C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer/ai_trainer'
list.files(file.path(wd, data_dir), recursive = F, full.names = F)
shiny::runApp('C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemreich/models/rnn_anomaly/ai_trainer')
runApp('C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemreich/models/rnn_anomaly/ai_trainer')
setwd("C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer")
runApp('ai_trainer')
file_list <- list.files(file.path(data_dir, date_stamp, 'unclassified'))
data_dir <- '../data/resmed/classification'
unclassified_path <- file.path(data_dir, date_stamp, 'unclassified')
normal_path <- file.path(data_dir, date_stamp, 'normal')
anomaly_path <- file.path(data_dir, date_stamp, 'anomaly')
file_list <- list.files(file.path(data_dir, date_stamp, 'unclassified'))
date_stamp = 20200219
data_dir <- '../data/resmed/classification'
unclassified_path <- file.path(data_dir, date_stamp, 'unclassified')
normal_path <- file.path(data_dir, date_stamp, 'normal')
anomaly_path <- file.path(data_dir, date_stamp, 'anomaly')
file_list <- list.files(file.path(data_dir, date_stamp, 'unclassified'))
file_list
unclassified_path
normal_path <- file.path(data_dir, date_stamp, 'normal')
unclassified_path <- file.path(data_dir, date_stamp, 'unclassified')
file_list <- list.files(file.path(data_dir, date_stamp, 'unclassified'))
unclassified_path
file_list <- list.files(file.path(data_dir, date_stamp, 'unclassified'))
file_list
wd = 'C:/Users/yvesm/OneDrive - Trivadis AG/projects/atemreich/atemteurer/ai_trainer'
setwd(wd)
file_list <- list.files(file.path(data_dir, date_stamp, 'unclassified'))
file_list
runApp()
runApp()
runApp()
runApp()
runApp()
list.files(unclassified_path)
list.files(data_dir)
list.files(data_dir, recursive = T, include.dirs = F)
length(list.files(data_dir, recursive = T, include.dirs = F)) / length(list.files(unclassified_path, recursive = T, include.dirs = F))
length(list.files(unclassified_path, recursive = T, include.dirs = F)) / length(list.files(data_dir, recursive = T, include.dirs = F))
length(list.files(unclassified_path, recursive = T, include.dirs = F))
length(list.files(data_dir, recursive = T, include.dirs = F))
[
data_dir
list.files(file.path(data_dir, input$date_stamp, 'unclassified'))
list.files(file.path(data_dir, '20200219', 'unclassified'))
length(list.files(file.path(data_dir, '20200219')))
length(list.files(file.path(data_dir, '20200219'), recursive = T, include.dirs = F))
length(list.files(unclassified_path, recursive = T, include.dirs = F)) / length(list.files(file.path(data_dir, '20200219'), recursive = T, include.dirs = F))
runApp()
